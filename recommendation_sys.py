# -*- coding: utf-8 -*-
"""Recommendation_Sys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WN1PAGrr6ndbe2gY5ePp_7F9UwFWqbJc

## **Mounting** **Drive**
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive

drive.mount("/gdrive")

# %cd ..
# %cd ..
# %cd ..
# %cd gdrive/'My Drive'
# %ls

# Importing the necessary libraries..

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Loading the datasets..
users = pd.read_csv('Lucid blog.csv/users.csv')
posts = pd.read_csv('Lucid blog.csv/posts.csv')
notifications = pd.read_csv('Lucid blog.csv/notifications.csv')

# Dropping empty columns.. Using copied data..

users_cp = users.copy()
users_cp.drop('password', axis=1, inplace=True)
users_cp.head(10)

# Knowing the data - dropping empty columns.. Using copied data..

posts_cp = posts.copy()
posts_cp.drop(['status_id','action','post_id'], axis=1, inplace=True)
posts_cp.head(10)

# Copying the data..

notifications_cp = notifications.copy()
notifications.head(10)

# Creating new DataFrame from copy sets..

data = pd.merge(users_cp, posts_cp, on='user_id')
datax = pd.merge(users_cp, notifications_cp, on='user_id')
data_cp = data.copy()
datax_cp = datax.copy()
data_cp.columns

datax_cp.columns

# Understanding the data..

data_cp.shape,datax_cp.shape

"""## **Detailed** **Report**"""

import pandas_profiling

pandas_profiling.ProfileReport(data_cp)

# Profile report - Users to notifications.. Followers.. e.t.c

pandas_profiling.ProfileReport(datax_cp)

"""## **Recommendation Begins**"""

# Calling neccesary libraries..

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
import seaborn as sb

# TF-IDF Vectorizer Object to remove all english stop words such as 'the', 'a'
tfidf = TfidfVectorizer(stop_words='english')

# Getting the TF-IDF frequency of a word occurring in a document e.g title of the post..
genome = tfidf.fit_transform(data_cp['content'])

genome.shape

# Computing similarity..

cosine_sim = linear_kernel(genome, genome)

indices = pd.Series(data_cp.index, index=data_cp['title']).drop_duplicates()


# System Complete..

"""## **System Testing**"""

# Building Test Function..

def recommend_posts_by(title, cosine_sim=cosine_sim):
    # Get the index of the movie that matches the title
    idx = indices[title]

    # Get the pairwsie similarity scores of all movies with that movie
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the movies based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar movies
    sim_scores = sim_scores[1:11]
    
    # Get the movie indices
    movie_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar movies
    return data_cp['title'].iloc[movie_indices]

# Testing the System..

recommend_posts_by(4)

